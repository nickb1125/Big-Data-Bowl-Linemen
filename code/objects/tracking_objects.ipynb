{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import plotly\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class master_track:\n",
    "    def __init__(self, track_paths, play_info_path, play_details_path, players_path):\n",
    "        self.d_line_pos = [\"NT\", \"DT\", \"MLB\", \"ILB\", \"LB\",  \"OLB\"]\n",
    "        self.o_line_pos = [\"T\", \"C\", \"G\"]\n",
    "        self.qb_pos = [\"QB\"]\n",
    "\n",
    "        player_df = pd.read_csv(players_path)\n",
    "        self.player_df = player_df\n",
    "        self.o_linemen_player_ids = self.player_df.loc[self.player_df['officialPosition'].isin(self.o_line_pos)].nflId.unique()\n",
    "        self.d_linemen_player_ids = self.player_df.loc[self.player_df['officialPosition'].isin(self.d_line_pos)].nflId.unique()\n",
    "        self.all_linemen = np.append(self.o_linemen_player_ids, self.d_linemen_player_ids)\n",
    "\n",
    "        self.play_info_df = pd.read_csv(play_info_path)\n",
    "        self.play_info_df['playId'] = (self.play_info_df['gameId'].apply(str) + self.play_info_df['playId'].apply(str)).apply(int)\n",
    "        self.play_details_df = pd.read_csv(play_details_path)\n",
    "        self.play_details_df['playId'] = (self.play_details_df['gameId'].apply(str) + self.play_details_df['playId'].apply(str)).apply(int)\n",
    "        self.play_details_df['disrupt_individual'] = (self.play_details_df.pff_hit + self.play_details_df.pff_hurry + self.play_details_df.pff_sack) > 0\n",
    "        team_dets = self.play_details_df.groupby('playId').agg({'disrupt_individual' : [sum]})\n",
    "        team_dets.columns = [\"_\".join(x) for x in np.array(team_dets.columns).ravel()]\n",
    "        team_dets = team_dets.reset_index()\n",
    "        team_dets['disrupt_team'] = team_dets.disrupt_individual_sum > 0\n",
    "        team_dets = team_dets.loc[:,['playId', 'disrupt_team']].drop_duplicates()\n",
    "        self.play_details_df = self.play_details_df.merge(team_dets, on = 'playId', how = 'left').copy()\n",
    "        self.max_time_after_snap = 7\n",
    "        \n",
    "        self.track_dfs = {}\n",
    "        week_num = 1\n",
    "        for week_path in track_paths:\n",
    "            week = pd.read_csv(week_path)\n",
    "            week['playId'] = (week['gameId'].apply(str) + week['playId'].apply(str)).apply(int)\n",
    "            week['week'] = week_num\n",
    "            snap_frames_df = week.loc[week.event == 'ball_snap', ['playId', 'frameId']].drop_duplicates().reset_index(drop  = True).rename(columns = {'frameId' : 'snap_frame'})\n",
    "            week = week.merge(snap_frames_df, on = 'playId', how = 'left')\n",
    "            week['time_after_snap'] = (week.frameId - week.snap_frame) * 0.1\n",
    "            no_snap = week.loc[week.time_after_snap.isnull()].drop_duplicates().playId.tolist()\n",
    "            week = week.loc[~(week.playId.isin(no_snap))]\n",
    "            self.track_dfs.update( {''.join(filter(lambda i: i.isdigit(), week_path)) : week} )\n",
    "            week_num += 1\n",
    "\n",
    "        self.individual_play_avgs = {key : None for key in self.track_dfs.keys()}\n",
    "        self.training_data_individual = {key : None for key in self.track_dfs.keys()}\n",
    "        self.training_data_team = {key : None for key in self.track_dfs.keys()}\n",
    "\n",
    "\n",
    "    def search_track_weeks(self, variables, variable_values):\n",
    "        needed = pd.DataFrame()\n",
    "        for week in range (len(self.track_dfs)):\n",
    "            week = str(week + 1)\n",
    "            curr_week = self.track_dfs.get(week)\n",
    "            if len(variables) == 2:\n",
    "                needed_info = curr_week.loc[(curr_week[variables[0]] == variable_values[0]) & (curr_week[variables[1]] == variable_values[1])]\n",
    "            else:\n",
    "                needed_info = curr_week.loc[(curr_week[variables[0]] == variable_values[0])]\n",
    "            needed = pd.concat([needed, needed_info])\n",
    "        return(needed.reset_index(drop = True))\n",
    "\n",
    "    def get_qb_track_on_play(self, play_id):\n",
    "        qb_play = self.play_details_df\n",
    "        qb_id = qb_play.loc[(qb_play.playId == play_id) & (qb_play.pff_positionLinedUp == 'QB')].reset_index(drop = 0).nflId[0]\n",
    "\n",
    "        qb_track = self.search_track_weeks(variables = [\"nflId\", \"playId\"], variable_values = [qb_id, play_id]).reset_index(drop = True)\n",
    "        return(qb_track)\n",
    "\n",
    "    def get_defender_and_qb_info_on_play(self, play_id):\n",
    "        # get qb and def data\n",
    "        # get def track\n",
    "        def_match = self.play_details_df.loc[(self.play_details_df.playId == play_id) & (self.play_details_df.pff_nflIdBlockedPlayer.notna()), ['playId', 'nflId', 'pff_nflIdBlockedPlayer']]\n",
    "        defender_track = []\n",
    "        for defender in def_match.nflId:\n",
    "            this_def = self.search_track_weeks(variables = [\"playId\", \"nflId\"], variable_values = [play_id, defender])\n",
    "            defender_track.append(this_def)\n",
    "        defender_track = pd.concat(defender_track).merge(def_match, on = ['playId', 'nflId'], how = 'left').loc[:,['nflId', 'playId', 'time_after_snap', 'pff_nflIdBlockedPlayer', 'x', 'y']].rename(columns = {'pff_nflIdBlockedPlayer' : 'nflId', 'nflId' : 'blockerId', 'x' : 'x_block', 'y' : 'y_block'})\n",
    "\n",
    "        # get qb track\n",
    "        qb_track = self.get_qb_track_on_play(play_id).loc[:, ['playId', 'time_after_snap', 'x', 'y', 's', 'a']].rename(columns = {'x' : 'x_qb', 'y' : 'y_qb', 's' : 's_qb', 'a' : 'a_qb'})\n",
    "        defender_qb_track = defender_track.merge(qb_track, on = ['playId', 'time_after_snap'], how = 'left')\n",
    "        defender_qb_track['blocker_distance_from_qb'] = np.sqrt( (defender_qb_track.x_qb - defender_qb_track.x_block)**2 + (defender_qb_track.y_qb - defender_qb_track.y_block)**2 )\n",
    "\n",
    "        dist_at_snap = defender_qb_track.loc[defender_qb_track.time_after_snap == 0, ['blockerId', 'blocker_distance_from_qb']].rename(columns = {'blocker_distance_from_qb' : 'distance_from_qb_at_snap'})\n",
    "        defender_qb_track = defender_qb_track.merge(dist_at_snap, on = 'blockerId', how = 'left')\n",
    "        defender_qb_track['blocker_distance_toward_qb_gained'] = defender_qb_track['distance_from_qb_at_snap'] - defender_qb_track['blocker_distance_from_qb']\n",
    "        def_qb_dat = defender_qb_track.drop(['distance_from_qb_at_snap'], axis = 1)\n",
    "\n",
    "        # get defender track wide\n",
    "\n",
    "        defender_track_wide = []\n",
    "        for column in ['blockerId', 'x_block', 'y_block', 'blocker_distance_from_qb', 'blocker_distance_toward_qb_gained']:\n",
    "            this_col_widen = def_qb_dat.groupby(['playId', 'time_after_snap', 'nflId'])[column].apply(lambda s: pd.Series(s.values, index=[f'{column}%s' % i for i in range(s.shape[0])])).unstack(-1).reset_index()\n",
    "            defender_track_wide.append(this_col_widen)\n",
    "        defender_track_wide = reduce(lambda x, y: pd.merge(x, y, on = ['playId', 'time_after_snap', 'nflId']), defender_track_wide).sort_values(['nflId', 'time_after_snap'])\n",
    "        def_qb_dat = def_qb_dat.drop(['blockerId', 'x_block', 'y_block', 'blocker_distance_from_qb', 'blocker_distance_toward_qb_gained'], axis = 1).merge(defender_track_wide, on = ['playId', 'time_after_snap', 'nflId'], how = 'left')\n",
    "\n",
    "        return(def_qb_dat)\n",
    "\n",
    "    def get_play(self, play_id):\n",
    "        def_qb_dat = self.get_defender_and_qb_info_on_play(play_id)\n",
    "        # separate qb and def data \n",
    "        qb_dat = def_qb_dat.loc[:,['time_after_snap', 'x_qb', 'y_qb', 's_qb', 'a_qb']].drop_duplicates()\n",
    "        def_dat = def_qb_dat.drop(['x_qb', 'y_qb', 's_qb', 'a_qb'], axis = 1)\n",
    "        #get pass rusher\n",
    "        rush_ids = self.play_details_df.loc[(self.play_details_df.pff_role == \"Pass Rush\") & (self.play_details_df.playId == play_id)].nflId.tolist()\n",
    "        play = self.search_track_weeks(variables = [\"playId\"], variable_values = [play_id])\n",
    "        play = play.loc[(play.nflId.isin(rush_ids))]\n",
    "        # create rush qb relationship data\n",
    "        play = play.merge(qb_dat, on = 'time_after_snap', how ='left')\n",
    "        play['rusher_distance_from_qb'] = np.sqrt( (play.x_qb - play.x)**2 + (play.y_qb - play.y)**2 )\n",
    "        dist_at_snap = play.loc[play.frameId == play.snap_frame, ['nflId', 'rusher_distance_from_qb']].rename(columns = {'rusher_distance_from_qb' : 'distance_from_qb_at_snap'})\n",
    "        play = play.merge(dist_at_snap, on = 'nflId', how = 'left')\n",
    "        play['rusher_distance_toward_qb_gained'] = play.distance_from_qb_at_snap - play.rusher_distance_from_qb\n",
    "        dis_gained_this_play_to_qb, change_in_velocity_this_play = [np.NaN], [np.NaN]\n",
    "        dis_gained_this_play_to_qb.extend(np.diff(play.rusher_distance_from_qb))\n",
    "        play['rusher_velocity_towards_qb'] = np.array(dis_gained_this_play_to_qb) / 0.1\n",
    "        change_in_velocity_this_play.extend(np.diff(play.rusher_velocity_towards_qb))\n",
    "        play['rusher_acceleration_towards_qb'] = np.array(change_in_velocity_this_play) / 0.1\n",
    "        #get rusher blocker data ralations\n",
    "        play_w_dup_blockers = play.merge(def_dat, on = ['playId', 'time_after_snap', 'nflId'], how = 'left')\n",
    "        play_w_dup_blockers.loc[play_w_dup_blockers.blockerId0.notna()]\n",
    "        all_blockers_accounted_for = False\n",
    "        id = 0\n",
    "        while all_blockers_accounted_for == False:\n",
    "            try:\n",
    "                play_w_dup_blockers['blocker_in_front' + str(id)] = (play_w_dup_blockers['blocker_distance_from_qb' + str(id)] - play_w_dup_blockers['rusher_distance_from_qb']) < 0\n",
    "                play_w_dup_blockers['blocker_distance_from_rusher' + str(id)] = np.sqrt( (play_w_dup_blockers['x_block' + str(id)] - play_w_dup_blockers['x'])**2 + (play_w_dup_blockers['y_block' + str(id)] - play_w_dup_blockers['y'])**2 )\n",
    "                id += 1\n",
    "            except:\n",
    "                all_blockers_accounted_for = True\n",
    "        play_w_dup_blockers['blockers_left'] = sum([play_w_dup_blockers['blocker_in_front' + str(this_id)] for this_id in range(id)])\n",
    "        play_w_dup_blockers['number_blockers_on_play'] = sum([play_w_dup_blockers['blockerId' + str(this_id)].notna() for this_id in range(id)])\n",
    "        [play_w_dup_blockers['blocker_distance_from_rusher' + str(this_id)] for this_id in range(id)]\n",
    "        play_w_dup_blockers['distance_of_closest_blocker_in_front'] = np.where(play_w_dup_blockers['blockers_left'] > 0, play_w_dup_blockers.loc[:,['blocker_distance_from_rusher' + str(this_id) for this_id in range(id)]].min(axis=1), 0)\n",
    "        play = play_w_dup_blockers.loc[play_w_dup_blockers.time_after_snap >= 0,[\n",
    "            'gameId', 'playId', 'nflId', 'time_after_snap', 'team', 'week',\n",
    "            'x', 'y', 'rusher_velocity_towards_qb', 'rusher_acceleration_towards_qb',\n",
    "            'x_qb', 'y_qb', 's_qb', 'a_qb',\n",
    "            'rusher_distance_from_qb', 'rusher_distance_toward_qb_gained',  'distance_of_closest_blocker_in_front', 'blockers_left', 'number_blockers_on_play']]\n",
    "        return(play)\n",
    "\n",
    "    def load_training_data(self, week):\n",
    "        if week < 2:\n",
    "            print(\"Must check for week after week 1 to have data\")\n",
    "            return(None)\n",
    "        needed_weeks = list(range(1, week))\n",
    "        for each_week in needed_weeks:\n",
    "            print(f'Getting week {each_week} training data')\n",
    "            if (str(each_week) in self.track_dfs.keys()) and not (self.training_data_individual.get(str(each_week)) is None) :\n",
    "                next\n",
    "            else:\n",
    "                all_individual_df = []\n",
    "                all_team_df = []\n",
    "                plays_this_week = np.unique(self.track_dfs.get(str(each_week)).playId).tolist()\n",
    "                for play_id in tqdm.tqdm(plays_this_week):\n",
    "                    play_individual_df = self.get_play(play_id = play_id)\n",
    "                    all_individual_df.append(play_individual_df)\n",
    "                all_individual_df = pd.concat(all_individual_df)\n",
    "                all_team_df = all_individual_df.groupby(['playId', 'time_after_snap'], as_index=False).agg({\n",
    "                    'nflId' : pd.Series.nunique,\n",
    "                    's_qb' : [min],\n",
    "                    'a_qb' : [min],\n",
    "                    'blockers_left' : [min, max, sum],\n",
    "                    'rusher_velocity_towards_qb' : [min, max, np.average],\n",
    "                    'rusher_acceleration_towards_qb' : [min, max, np.average],\n",
    "                    \"rusher_distance_from_qb\": [min, max, np.average],\n",
    "                    \"rusher_distance_toward_qb_gained\": [min, max, np.average],\n",
    "                    \"distance_of_closest_blocker_in_front\": [min, max, np.average]})\n",
    "                all_team_df.columns = [\"_\".join(x) for x in np.array(all_team_df.columns).ravel()]\n",
    "                all_team_df = all_team_df.rename(columns = {'playId_' : 'playId', 'time_after_snap_' : 'time_after_snap'}).rename(columns = {'nflId_nunique' : 'n_rushers', 's_qb_min' : 's_qb', 'a_qb_min' : 'a_qb'})\n",
    "                all_team_df['week'] = each_week\n",
    "\n",
    "                results_data = self.play_details_df.fillna(0)\n",
    "                results_data_ind = results_data.loc[:, ['nflId', 'playId', 'pff_positionLinedUp', 'disrupt_individual']]\n",
    "                results_data_ind.pff_positionLinedUp = np.select(\n",
    "                            [\n",
    "                                (['E' in position for position in results_data_ind.pff_positionLinedUp]), \n",
    "                                (['B' in position for position in results_data_ind.pff_positionLinedUp]),\n",
    "                                (['T' in position for position in results_data_ind.pff_positionLinedUp])\n",
    "\n",
    "                            ], \n",
    "                            [\n",
    "                                'End', \n",
    "                                'Back',\n",
    "                                'Tackle'\n",
    "\n",
    "                            ], \n",
    "                            default='Non-Lineman'\n",
    "                        )\n",
    "                results_data_team = results_data.loc[:, ['playId', 'disrupt_team']].drop_duplicates()\n",
    "                training_df_individual = all_individual_df.merge(results_data_ind, on = ['nflId', 'playId'], how = 'left')\n",
    "                training_df_team = all_team_df.merge(results_data_team, on = ['playId'], how = 'left')\n",
    "                \n",
    "                self.training_data_individual[str(each_week)] = training_df_individual\n",
    "                self.training_data_team[str(each_week)] = training_df_team\n",
    "        print('Done')\n",
    "\n",
    "\n",
    "    def get_averages_up_to_week(self, week):\n",
    "        if week < 2:\n",
    "            print(\"Must check for week after week 1 to have data\")\n",
    "            return(None)\n",
    "        needed_weeks = list(range(1, week))\n",
    "        progressive_training_indivudual = []\n",
    "        for each_week in needed_weeks:\n",
    "            print(f'Getting week {each_week} average metrics by position and blocker number')\n",
    "            if (str(each_week) in self.track_dfs.keys()) and not (self.training_data_individual.get(str(each_week)) is None) :\n",
    "                next\n",
    "            try:\n",
    "                this_week_train_dat = self.training_data_individual.get(str(each_week))\n",
    "            except:\n",
    "                print(f'Training data has not been loaded for week {str(each_week)}')\n",
    "                return(None)\n",
    "            progressive_training_indivudual.append(this_week_train_dat)\n",
    "            all_before_this_week = pd.concat(progressive_training_indivudual)  \n",
    "            player_averages = all_before_this_week.groupby(['time_after_snap', 'number_blockers_on_play', 'pff_positionLinedUp']).agg({\n",
    "                                                            'nflId' : pd.Series.nunique,\n",
    "                                                            'blockers_left' : [np.median],\n",
    "                                                            'rusher_velocity_towards_qb' : [np.average],\n",
    "                                                            'rusher_acceleration_towards_qb' : [np.average],\n",
    "                                                            \"rusher_distance_from_qb\": [np.average],\n",
    "                                                            \"rusher_distance_toward_qb_gained\": [np.average],\n",
    "                                                            \"distance_of_closest_blocker_in_front\": [np.average]\n",
    "                                                            })\n",
    "            player_averages.columns = [\"_\".join(x) for x in np.array(player_averages.columns).ravel()]\n",
    "            player_averages = player_averages.rename(columns = {'time_after_snap_' : 'time_after_snap'}).rename(columns = {'nflId_nunique' : 'n_rushers'}).reset_index()\n",
    "            self.individual_play_avgs[str(each_week)] = player_averages\n",
    "        print('Done')\n",
    "\n",
    "    def get_rush_sequences_labels(self, week, normalize = True):\n",
    "        needed_weeks = list(range(1, week))\n",
    "        progressive_training_team = []\n",
    "        for each_week in needed_weeks:\n",
    "            if normalize == True:\n",
    "                print(f'Getting week {each_week} normalized training data')\n",
    "            else:\n",
    "                print(f'Getting week {each_week} (unnormalized) training data')\n",
    "            try:\n",
    "                this_week_train_dat = self.training_data_team.get(str(each_week))\n",
    "            except:\n",
    "                print(f'Training data has not been loaded for week {str(each_week)}')\n",
    "            progressive_training_team.append(this_week_train_dat)\n",
    "\n",
    "        all_training = pd.concat(progressive_training_team)\n",
    "        all_training\n",
    "        if normalize == True:\n",
    "                scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "                scale_cols = all_training.drop(['time_after_snap', 'playId', 'week', 'disrupt_team'], axis = 1)\n",
    "                non_scale_cols = all_training.loc[:,['time_after_snap', 'playId', 'week', 'disrupt_team']]\n",
    "                columns = scale_cols.columns\n",
    "                scale_cols = pd.DataFrame(scaler.fit_transform(scale_cols))\n",
    "                scale_cols.columns = columns\n",
    "                all_training = pd.concat([non_scale_cols.reset_index(drop = 1), scale_cols.reset_index(drop = 1)], axis = 1)\n",
    "\n",
    "        sequences = []\n",
    "        for playId, group in tqdm.tqdm(all_training.groupby(\"playId\")):\n",
    "            label = group.iloc[0].disrupt_team\n",
    "            sequence_features = group.drop(['time_after_snap', 'playId', 'week', 'disrupt_team'], axis = 1)\n",
    "            sequences.append((sequence_features, label))\n",
    "        return(sequences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def split_training_times(self, week, normalize = False):\n",
    "        ### define split all train data to split all training data up to week into times (store in dictionary)\n",
    "\n",
    "        needed_weeks = list(range(1, week))\n",
    "        progressive_training_team = []\n",
    "        for each_week in needed_weeks:\n",
    "            if normalize == True:\n",
    "                print(f'Getting week {each_week} normalized training data')\n",
    "            else:\n",
    "                print(f'Getting week {each_week} (unnormalized) training data')\n",
    "            try:\n",
    "                this_week_train_dat = self.training_data_team.get(str(each_week))\n",
    "            except:\n",
    "                print(f'Training data has not been loaded for week {str(each_week)}')\n",
    "                return(None)\n",
    "            progressive_training_team.append(this_week_train_dat)\n",
    "\n",
    "\n",
    "        train_before_this_week = pd.concat(progressive_training_team).drop(['playId'], axis = 1)\n",
    "        times_split_dict = {str(round(time, 1)) : train_before_this_week.loc[train_before_this_week.time_after_snap == time].drop(['time_after_snap', 'week'], axis = 1) for time in train_before_this_week.time_after_snap.unique()}\n",
    "\n",
    "        if normalize == True:\n",
    "            for time_key in times_split_dict.keys():\n",
    "                df_at_time = times_split_dict.get(time_key)\n",
    "                disrupt_team = df_at_time.disrupt_team\n",
    "                df_at_time = df_at_time.drop(['disrupt_team'], axis = 1)\n",
    "                min_max_df=(df_at_time-df_at_time.mean())/df_at_time.std()\n",
    "                min_max_df['disrupt_team'] = disrupt_team\n",
    "                times_split_dict[time_key] = min_max_df\n",
    "        print('Done')\n",
    "        return(times_split_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_play_train(self, play_id, type = 'individual', time_split = False):\n",
    "        if (type == 'individual') & (time_split == True):\n",
    "            print('Time split is only used for team level data. Changing time_split to False.')\n",
    "            time_split = False\n",
    "\n",
    "        get_play_week = self.search_track_weeks(variables = ['playId'], variable_values = [play_id]).week.tolist()[0]\n",
    "        if type == 'individual':\n",
    "            all_training = self.training_data_individual.get(str(get_play_week))\n",
    "        else:\n",
    "            all_training = self.training_data_team.get(str(get_play_week))\n",
    "        current_play = all_training.loc[(all_training.playId == play_id)]\n",
    "        if time_split == True:\n",
    "            current_play = current_play.drop(['playId'], axis = 1)\n",
    "            times_split_dict = {str(round(time, 1)) : current_play.loc[current_play.time_after_snap == time].drop(['time_after_snap', 'week'], axis = 1) for time in current_play.time_after_snap.unique()}\n",
    "            current_play = times_split_dict\n",
    "        return(current_play)\n",
    "\n",
    "    def predict_play(mods, play_id):\n",
    "        data_predict = self.get_play_train(play_id, type = 'team', time_split = True)\n",
    "        predictions = []\n",
    "        for key in data_predict.keys():\n",
    "            if mods.get(str(key)) == None:\n",
    "                print('Not enough models trained to cover entire play length')\n",
    "                missing_len = round((max([float(x) for x in data_predict.keys()]) - (float(key) - 0.1)) / 0.1, 0)\n",
    "                missing_append = np.repeat(np.NaN, missing_len).tolist()\n",
    "                predictions = predictions + missing_append\n",
    "                break\n",
    "            mod_time = mods.get(str(key))\n",
    "            data_predict_time = data_predict.get(str(key))\n",
    "            data_predict_true_labels = [int(x) for x in data_predict_time.disrupt_team]\n",
    "            data_predict_data = data_predict_time.drop(['disrupt_team'], axis = 1)\n",
    "            predicted = mod_time.predict_proba(data_predict_data)[0][1]\n",
    "            predictions.append(predicted)\n",
    "        predictions_df = pd.DataFrame({'time_after_snap' : data_predict.keys(), 'predicted_prob' : predictions})\n",
    "        return(predictions_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def replace_player_with_average(self, play_id, player_id): # might need to add normalize to this\n",
    "        current_play = self.get_play_train(play_id, type = 'individual', time_split = False)\n",
    "        current_player = current_play.loc[(all_training.nflId == player_id)].copy()\n",
    "        current_player_keep = current_player.loc[:,['gameId', 'playId', 'nflId', 'team', 'week', 'x', \n",
    "                                                    'y', 'x_qb', 'y_qb', 's_qb', 'a_qb', 'disrupt_individual']].reset_index(drop = 1)\n",
    "        max_time_after_snap = max(current_player.time_after_snap.tolist())\n",
    "        pos = current_player.pff_positionLinedUp.tolist()[0]\n",
    "        num_blockers = current_player.number_blockers_on_play.tolist()[0]\n",
    "        replace_averages = averages_before_this_week.loc[(averages_before_this_week.number_blockers_on_play == num_blockers) & \n",
    "                                                            (averages_before_this_week.pff_positionLinedUp == pos) &\n",
    "                                                            (averages_before_this_week.time_after_snap <= max_time_after_snap)].reset_index(drop = 1).rename({'blockers_left_median' : 'blockers_left', \n",
    "                                                            'rusher_velocity_towards_qb_average' : 'rusher_velocity_towards_qb',\n",
    "                                                            'rusher_acceleration_towards_qb_average' : 'rusher_acceleration_towards_qb', \n",
    "                                                            'rusher_distance_from_qb_average' : 'rusher_distance_from_qb', \n",
    "                                                            'rusher_distance_toward_qb_gained_average' : 'rusher_distance_toward_qb_gained',\n",
    "                                                            'distance_of_closest_blocker_in_front_average' : 'distance_of_closest_blocker_in_front'}, axis = 1)\n",
    "\n",
    "        replace_averages = pd.concat([replace_averages, current_player_keep], axis = 1).reset_index(drop = 1)\n",
    "        removed_player = current_play.loc[~(current_play.nflId == player_id)].reset_index(drop = 1)\n",
    "        replaced = pd.concat([replace_averages, removed_player])\n",
    "        replaced_team = replaced.groupby(['playId', 'time_after_snap'], as_index=False).agg({\n",
    "                            'nflId' : pd.Series.nunique,\n",
    "                            's_qb' : [min],\n",
    "                            'a_qb' : [min],\n",
    "                            'blockers_left' : [min, max, sum],\n",
    "                            'rusher_velocity_towards_qb' : [min, max, np.average],\n",
    "                            'rusher_acceleration_towards_qb' : [min, max, np.average],\n",
    "                            \"rusher_distance_from_qb\": [min, max, np.average],\n",
    "                            \"rusher_distance_toward_qb_gained\": [min, max, np.average],\n",
    "                            \"distance_of_closest_blocker_in_front\": [min, max, np.average],\n",
    "                            \"disrupt_individual\" : [sum],\n",
    "                            \"week\" : [min]})\n",
    "        replaced_team.columns = [\"_\".join(x) for x in np.array(replaced_team.columns).ravel()]\n",
    "        replaced_team['disrupt_individual_sum'] = replaced_team['disrupt_individual_sum'] > 1\n",
    "        replaced_team = replaced_team.rename(columns = {'playId_' : 'playId', 'time_after_snap_' : 'time_after_snap'}).rename(columns = {'nflId_nunique' : 'n_rushers', \n",
    "                                                                                                                                        's_qb_min' : 's_qb', 'a_qb_min' : 'a_qb',\n",
    "                                                                                                                                        'disrupt_individual_sum' : 'disrupt_team',\n",
    "                                                                                                                                        'week_min' : 'week'})\n",
    "        return(replaced_team)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_metric_averages(self, week):\n",
    "        if self.overall_avgs.get(str(week - 1)) is None:\n",
    "            print(\"Rusher averages are not set to week of this play. Use the load_distance_averages_by_time_after_snap to load proper week averages\")\n",
    "            return(None)\n",
    "        overall_avgs = self.overall_avgs.get(str(week))\n",
    "        plt.plot(overall_avgs.time_after_snap.values, overall_avgs.rusher_distance_to_qb_gained_league_avg.values)\n",
    "        plt.show()\n",
    "\n",
    "    def return_team_game_model_training(self, week):\n",
    "        return('none')\n",
    "        # finish\n",
    "\n",
    "\n",
    "class mod_load:\n",
    "    def __init__(self, time_split_data):\n",
    "        self.time_split_data = time_split_data\n",
    "    \n",
    "    def grad_boost(self):\n",
    "        # train xgbooster on each time\n",
    "        times = [float(time_key) for time_key in self.time_split_data.keys()]\n",
    "        gb_classifiers = {}\n",
    "        for time in times:\n",
    "            this_time_dat = self.time_split_data.get(str(time))\n",
    "            train_all, test_all =  this_time_dat.drop(['disrupt_team'], axis = 1), [int(x) for x in this_time_dat.disrupt_team]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_all, test_all, random_state=42)\n",
    "            if len(y_test) < 200 :\n",
    "                break\n",
    "            print(f'Seconds after snap : {time}')\n",
    "            print(f'Train set N : {len(y_train)}')\n",
    "            print(f'Test set N : {len(y_test)}')\n",
    "            xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"logloss\")\n",
    "            xgb_model.fit(X_train, y_train)\n",
    "            y_pred = xgb_model.predict(X_test)\n",
    "            print(f'AUC : {sklearn.metrics.roc_auc_score(y_test,y_pred)}')\n",
    "            print('---------------------------------------')\n",
    "            gb_classifiers.update({str(time) : xgb_model})\n",
    "        return(gb_classifiers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# to do:\n",
    "# think about the fact that metric must be team based and must incoorperate number of rushers\n",
    "# fix updater (ie. week 3 or above)\n",
    "# add qb to tracking plots\n",
    "# create modeling for each play frame\n",
    "# add model prob of event to plots\n",
    "# create funciton to get player metric after week x\n",
    "# create function to get team metric for week x\n",
    "# create function to get result for number of team events and training data\n",
    "# create modeling for team number of events by metric\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b5140b98f9aa636ad904647e184ddf94a8c49b25e448223e4f659e3845abf7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
