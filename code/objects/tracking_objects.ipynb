{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import plotly\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 422>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=411'>412</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m(replaced_team)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=416'>417</a>\u001b[0m \u001b[39m####### modeling objects\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=417'>418</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=418'>419</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=419'>420</a>\u001b[0m \u001b[39m## make dataset object\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=421'>422</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRushDataset\u001b[39;00m(Dataset):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=422'>423</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, sequences, sequence_length):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/nickbachelder/Desktop/Kaggle/Linemen/code/objects/tracking_objects.ipynb#W1sZmlsZQ%3D%3D?line=423'>424</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msequences \u001b[39m=\u001b[39m sequences\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class master_track:\n",
    "    def __init__(self, track_paths, play_info_path, play_details_path, players_path):\n",
    "        self.d_line_pos = [\"NT\", \"DT\", \"MLB\", \"ILB\", \"LB\",  \"OLB\"]\n",
    "        self.o_line_pos = [\"T\", \"C\", \"G\"]\n",
    "        self.qb_pos = [\"QB\"]\n",
    "\n",
    "        player_df = pd.read_csv(players_path)\n",
    "        self.player_df = player_df\n",
    "        self.o_linemen_player_ids = self.player_df.loc[self.player_df['officialPosition'].isin(self.o_line_pos)].nflId.unique()\n",
    "        self.d_linemen_player_ids = self.player_df.loc[self.player_df['officialPosition'].isin(self.d_line_pos)].nflId.unique()\n",
    "        self.all_linemen = np.append(self.o_linemen_player_ids, self.d_linemen_player_ids)\n",
    "\n",
    "        self.play_info_df = pd.read_csv(play_info_path)\n",
    "        self.play_info_df['playId'] = (self.play_info_df['gameId'].apply(str) + self.play_info_df['playId'].apply(str)).apply(int)\n",
    "        self.play_details_df = pd.read_csv(play_details_path)\n",
    "        self.play_details_df['playId'] = (self.play_details_df['gameId'].apply(str) + self.play_details_df['playId'].apply(str)).apply(int)\n",
    "        self.play_details_df['disrupt_individual'] = (self.play_details_df.pff_hit + self.play_details_df.pff_hurry + self.play_details_df.pff_sack) > 0\n",
    "        team_dets = self.play_details_df.groupby('playId').agg({'disrupt_individual' : [sum]})\n",
    "        team_dets.columns = [\"_\".join(x) for x in np.array(team_dets.columns).ravel()]\n",
    "        team_dets = team_dets.reset_index()\n",
    "        team_dets['disrupt_team'] = team_dets.disrupt_individual_sum > 0\n",
    "        team_dets = team_dets.loc[:,['playId', 'disrupt_team']].drop_duplicates()\n",
    "        self.play_details_df = self.play_details_df.merge(team_dets, on = 'playId', how = 'left').copy()\n",
    "        self.max_time_after_snap = 7\n",
    "        \n",
    "        self.track_dfs = {}\n",
    "        week_num = 1\n",
    "        for week_path in track_paths:\n",
    "            week = pd.read_csv(week_path)\n",
    "            week['playId'] = (week['gameId'].apply(str) + week['playId'].apply(str)).apply(int)\n",
    "            week['week'] = week_num\n",
    "            snap_frames_df = week.loc[week.event == 'ball_snap', ['playId', 'frameId']].drop_duplicates().reset_index(drop  = True).rename(columns = {'frameId' : 'snap_frame'})\n",
    "            week = week.merge(snap_frames_df, on = 'playId', how = 'left')\n",
    "            week['time_after_snap'] = (week.frameId - week.snap_frame) * 0.1\n",
    "            no_snap = week.loc[week.time_after_snap.isnull()].drop_duplicates().playId.tolist()\n",
    "            week = week.loc[~(week.playId.isin(no_snap))]\n",
    "            self.track_dfs.update( {''.join(filter(lambda i: i.isdigit(), week_path)) : week} )\n",
    "            week_num += 1\n",
    "\n",
    "        self.individual_play_avgs = {key : None for key in self.track_dfs.keys()}\n",
    "        self.training_data_individual = {key : None for key in self.track_dfs.keys()}\n",
    "        self.training_data_team = {key : None for key in self.track_dfs.keys()}\n",
    "\n",
    "\n",
    "    def search_track_weeks(self, variables, variable_values):\n",
    "        needed = pd.DataFrame()\n",
    "        for week in range (len(self.track_dfs)):\n",
    "            week = str(week + 1)\n",
    "            curr_week = self.track_dfs.get(week)\n",
    "            if len(variables) == 2:\n",
    "                needed_info = curr_week.loc[(curr_week[variables[0]] == variable_values[0]) & (curr_week[variables[1]] == variable_values[1])]\n",
    "            else:\n",
    "                needed_info = curr_week.loc[(curr_week[variables[0]] == variable_values[0])]\n",
    "            needed = pd.concat([needed, needed_info])\n",
    "        return(needed.reset_index(drop = True))\n",
    "\n",
    "    def get_qb_track_on_play(self, play_id):\n",
    "        qb_play = self.play_details_df\n",
    "        qb_id = qb_play.loc[(qb_play.playId == play_id) & (qb_play.pff_positionLinedUp == 'QB')].reset_index(drop = 0).nflId[0]\n",
    "\n",
    "        qb_track = self.search_track_weeks(variables = [\"nflId\", \"playId\"], variable_values = [qb_id, play_id]).reset_index(drop = True)\n",
    "        return(qb_track)\n",
    "\n",
    "    def get_defender_and_qb_info_on_play(self, play_id):\n",
    "        # get qb and def data\n",
    "        # get def track\n",
    "        def_match = self.play_details_df.loc[(self.play_details_df.playId == play_id) & (self.play_details_df.pff_nflIdBlockedPlayer.notna()), ['playId', 'nflId', 'pff_nflIdBlockedPlayer']]\n",
    "        defender_track = []\n",
    "        for defender in def_match.nflId:\n",
    "            this_def = self.search_track_weeks(variables = [\"playId\", \"nflId\"], variable_values = [play_id, defender])\n",
    "            defender_track.append(this_def)\n",
    "        defender_track = pd.concat(defender_track).merge(def_match, on = ['playId', 'nflId'], how = 'left').loc[:,['nflId', 'playId', 'time_after_snap', 'pff_nflIdBlockedPlayer', 'x', 'y']].rename(columns = {'pff_nflIdBlockedPlayer' : 'nflId', 'nflId' : 'blockerId', 'x' : 'x_block', 'y' : 'y_block'})\n",
    "\n",
    "        # get qb track\n",
    "        qb_track = self.get_qb_track_on_play(play_id).loc[:, ['playId', 'time_after_snap', 'x', 'y', 's', 'a']].rename(columns = {'x' : 'x_qb', 'y' : 'y_qb', 's' : 's_qb', 'a' : 'a_qb'})\n",
    "        defender_qb_track = defender_track.merge(qb_track, on = ['playId', 'time_after_snap'], how = 'left')\n",
    "        defender_qb_track['blocker_distance_from_qb'] = np.sqrt( (defender_qb_track.x_qb - defender_qb_track.x_block)**2 + (defender_qb_track.y_qb - defender_qb_track.y_block)**2 )\n",
    "\n",
    "        dist_at_snap = defender_qb_track.loc[defender_qb_track.time_after_snap == 0, ['blockerId', 'blocker_distance_from_qb']].rename(columns = {'blocker_distance_from_qb' : 'distance_from_qb_at_snap'})\n",
    "        defender_qb_track = defender_qb_track.merge(dist_at_snap, on = 'blockerId', how = 'left')\n",
    "        defender_qb_track['blocker_distance_toward_qb_gained'] = defender_qb_track['distance_from_qb_at_snap'] - defender_qb_track['blocker_distance_from_qb']\n",
    "        def_qb_dat = defender_qb_track.drop(['distance_from_qb_at_snap'], axis = 1)\n",
    "\n",
    "        # get defender track wide\n",
    "\n",
    "        defender_track_wide = []\n",
    "        for column in ['blockerId', 'x_block', 'y_block', 'blocker_distance_from_qb', 'blocker_distance_toward_qb_gained']:\n",
    "            this_col_widen = def_qb_dat.groupby(['playId', 'time_after_snap', 'nflId'])[column].apply(lambda s: pd.Series(s.values, index=[f'{column}%s' % i for i in range(s.shape[0])])).unstack(-1).reset_index()\n",
    "            defender_track_wide.append(this_col_widen)\n",
    "        defender_track_wide = reduce(lambda x, y: pd.merge(x, y, on = ['playId', 'time_after_snap', 'nflId']), defender_track_wide).sort_values(['nflId', 'time_after_snap'])\n",
    "        def_qb_dat = def_qb_dat.drop(['blockerId', 'x_block', 'y_block', 'blocker_distance_from_qb', 'blocker_distance_toward_qb_gained'], axis = 1).merge(defender_track_wide, on = ['playId', 'time_after_snap', 'nflId'], how = 'left')\n",
    "\n",
    "        return(def_qb_dat)\n",
    "\n",
    "    def get_play(self, play_id):\n",
    "        def_qb_dat = self.get_defender_and_qb_info_on_play(play_id)\n",
    "        # separate qb and def data \n",
    "        qb_dat = def_qb_dat.loc[:,['time_after_snap', 'x_qb', 'y_qb', 's_qb', 'a_qb']].drop_duplicates()\n",
    "        def_dat = def_qb_dat.drop(['x_qb', 'y_qb', 's_qb', 'a_qb'], axis = 1)\n",
    "        #get pass rusher\n",
    "        rush_ids = self.play_details_df.loc[(self.play_details_df.pff_role == \"Pass Rush\") & (self.play_details_df.playId == play_id)].nflId.tolist()\n",
    "        play = self.search_track_weeks(variables = [\"playId\"], variable_values = [play_id])\n",
    "        play = play.loc[(play.nflId.isin(rush_ids))]\n",
    "        # create rush qb relationship data\n",
    "        play = play.merge(qb_dat, on = 'time_after_snap', how ='left')\n",
    "        play['rusher_distance_from_qb'] = np.sqrt( (play.x_qb - play.x)**2 + (play.y_qb - play.y)**2 )\n",
    "        dist_at_snap = play.loc[play.frameId == play.snap_frame, ['nflId', 'rusher_distance_from_qb']].rename(columns = {'rusher_distance_from_qb' : 'distance_from_qb_at_snap'})\n",
    "        play = play.merge(dist_at_snap, on = 'nflId', how = 'left')\n",
    "        play['rusher_distance_toward_qb_gained'] = play.distance_from_qb_at_snap - play.rusher_distance_from_qb\n",
    "        dis_gained_this_play_to_qb, change_in_velocity_this_play = [np.NaN], [np.NaN]\n",
    "        dis_gained_this_play_to_qb.extend(np.diff(play.rusher_distance_from_qb))\n",
    "        play['rusher_velocity_towards_qb'] = np.array(dis_gained_this_play_to_qb) / 0.1\n",
    "        change_in_velocity_this_play.extend(np.diff(play.rusher_velocity_towards_qb))\n",
    "        play['rusher_acceleration_towards_qb'] = np.array(change_in_velocity_this_play) / 0.1\n",
    "        #get rusher blocker data ralations\n",
    "        play_w_dup_blockers = play.merge(def_dat, on = ['playId', 'time_after_snap', 'nflId'], how = 'left')\n",
    "        play_w_dup_blockers.loc[play_w_dup_blockers.blockerId0.notna()]\n",
    "        all_blockers_accounted_for = False\n",
    "        id = 0\n",
    "        while all_blockers_accounted_for == False:\n",
    "            try:\n",
    "                play_w_dup_blockers['blocker_in_front' + str(id)] = (play_w_dup_blockers['blocker_distance_from_qb' + str(id)] - play_w_dup_blockers['rusher_distance_from_qb']) < 0\n",
    "                play_w_dup_blockers['blocker_distance_from_rusher' + str(id)] = np.sqrt( (play_w_dup_blockers['x_block' + str(id)] - play_w_dup_blockers['x'])**2 + (play_w_dup_blockers['y_block' + str(id)] - play_w_dup_blockers['y'])**2 )\n",
    "                id += 1\n",
    "            except:\n",
    "                all_blockers_accounted_for = True\n",
    "        play_w_dup_blockers['blockers_left'] = sum([play_w_dup_blockers['blocker_in_front' + str(this_id)] for this_id in range(id)])\n",
    "        play_w_dup_blockers['number_blockers_on_play'] = sum([play_w_dup_blockers['blockerId' + str(this_id)].notna() for this_id in range(id)])\n",
    "        [play_w_dup_blockers['blocker_distance_from_rusher' + str(this_id)] for this_id in range(id)]\n",
    "        play_w_dup_blockers['distance_of_closest_blocker_in_front'] = np.where(play_w_dup_blockers['blockers_left'] > 0, play_w_dup_blockers.loc[:,['blocker_distance_from_rusher' + str(this_id) for this_id in range(id)]].min(axis=1), 0)\n",
    "        play = play_w_dup_blockers.loc[play_w_dup_blockers.time_after_snap >= 0,[\n",
    "            'gameId', 'playId', 'nflId', 'time_after_snap', 'team', 'week',\n",
    "            'x', 'y', 'rusher_velocity_towards_qb', 'rusher_acceleration_towards_qb',\n",
    "            'x_qb', 'y_qb', 's_qb', 'a_qb',\n",
    "            'rusher_distance_from_qb', 'rusher_distance_toward_qb_gained',  'distance_of_closest_blocker_in_front', 'blockers_left', 'number_blockers_on_play']]\n",
    "        return(play)\n",
    "\n",
    "    def load_training_data(self, week):\n",
    "        if week < 2:\n",
    "            print(\"Must check for week after week 1 to have data\")\n",
    "            return(None)\n",
    "        needed_weeks = list(range(1, week))\n",
    "        for each_week in needed_weeks:\n",
    "            print(f'Getting week {each_week} training data')\n",
    "            if (str(each_week) in self.track_dfs.keys()) and not (self.training_data_individual.get(str(each_week)) is None) :\n",
    "                next\n",
    "            else:\n",
    "                all_individual_df = []\n",
    "                all_team_df = []\n",
    "                plays_this_week = np.unique(self.track_dfs.get(str(each_week)).playId).tolist()\n",
    "                for play_id in tqdm.tqdm(plays_this_week):\n",
    "                    play_individual_df = self.get_play(play_id = play_id)\n",
    "                    all_individual_df.append(play_individual_df)\n",
    "                all_individual_df = pd.concat(all_individual_df)\n",
    "                all_team_df = all_individual_df.groupby(['playId', 'time_after_snap'], as_index=False).agg({\n",
    "                    'nflId' : pd.Series.nunique,\n",
    "                    's_qb' : [min],\n",
    "                    'a_qb' : [min],\n",
    "                    'blockers_left' : [min, max, sum],\n",
    "                    'rusher_velocity_towards_qb' : [min, max, np.average],\n",
    "                    'rusher_acceleration_towards_qb' : [min, max, np.average],\n",
    "                    \"rusher_distance_from_qb\": [min, max, np.average],\n",
    "                    \"rusher_distance_toward_qb_gained\": [min, max, np.average],\n",
    "                    \"distance_of_closest_blocker_in_front\": [min, max, np.average]})\n",
    "                all_team_df.columns = [\"_\".join(x) for x in np.array(all_team_df.columns).ravel()]\n",
    "                all_team_df = all_team_df.rename(columns = {'playId_' : 'playId', 'time_after_snap_' : 'time_after_snap'}).rename(columns = {'nflId_nunique' : 'n_rushers', 's_qb_min' : 's_qb', 'a_qb_min' : 'a_qb'})\n",
    "                all_team_df['week'] = each_week\n",
    "\n",
    "                results_data = self.play_details_df.fillna(0)\n",
    "                results_data_ind = results_data.loc[:, ['nflId', 'playId', 'pff_positionLinedUp', 'disrupt_individual']]\n",
    "                results_data_ind.pff_positionLinedUp = np.select(\n",
    "                            [\n",
    "                                (['E' in position for position in results_data_ind.pff_positionLinedUp]), \n",
    "                                (['B' in position for position in results_data_ind.pff_positionLinedUp]),\n",
    "                                (['T' in position for position in results_data_ind.pff_positionLinedUp])\n",
    "\n",
    "                            ], \n",
    "                            [\n",
    "                                'End', \n",
    "                                'Back',\n",
    "                                'Tackle'\n",
    "\n",
    "                            ], \n",
    "                            default='Non-Lineman'\n",
    "                        )\n",
    "                results_data_team = results_data.loc[:, ['playId', 'disrupt_team']].drop_duplicates()\n",
    "                training_df_individual = all_individual_df.merge(results_data_ind, on = ['nflId', 'playId'], how = 'left')\n",
    "                training_df_team = all_team_df.merge(results_data_team, on = ['playId'], how = 'left')\n",
    "                \n",
    "                self.training_data_individual[str(each_week)] = training_df_individual\n",
    "                self.training_data_team[str(each_week)] = training_df_team\n",
    "        print('Done')\n",
    "\n",
    "\n",
    "    def get_averages_up_to_week(self, week):\n",
    "        if week < 2:\n",
    "            print(\"Must check for week after week 1 to have data\")\n",
    "            return(None)\n",
    "        needed_weeks = list(range(1, week))\n",
    "        progressive_training_indivudual = []\n",
    "        for each_week in needed_weeks:\n",
    "            print(f'Getting week {each_week} average metrics by position and blocker number')\n",
    "            if (str(each_week) in self.track_dfs.keys()) and not (self.training_data_individual.get(str(each_week)) is None) :\n",
    "                next\n",
    "            try:\n",
    "                this_week_train_dat = self.training_data_individual.get(str(each_week))\n",
    "            except:\n",
    "                print(f'Training data has not been loaded for week {str(each_week)}')\n",
    "                return(None)\n",
    "            progressive_training_indivudual.append(this_week_train_dat)\n",
    "            all_before_this_week = pd.concat(progressive_training_indivudual)  \n",
    "            player_averages = all_before_this_week.groupby(['time_after_snap', 'number_blockers_on_play', 'pff_positionLinedUp']).agg({\n",
    "                                                            'nflId' : pd.Series.nunique,\n",
    "                                                            'blockers_left' : [np.median],\n",
    "                                                            'rusher_velocity_towards_qb' : [np.average],\n",
    "                                                            'rusher_acceleration_towards_qb' : [np.average],\n",
    "                                                            \"rusher_distance_from_qb\": [np.average],\n",
    "                                                            \"rusher_distance_toward_qb_gained\": [np.average],\n",
    "                                                            \"distance_of_closest_blocker_in_front\": [np.average]\n",
    "                                                            })\n",
    "            player_averages.columns = [\"_\".join(x) for x in np.array(player_averages.columns).ravel()]\n",
    "            player_averages = player_averages.rename(columns = {'time_after_snap_' : 'time_after_snap'}).rename(columns = {'nflId_nunique' : 'n_rushers'}).reset_index()\n",
    "            self.individual_play_avgs[str(each_week)] = player_averages\n",
    "        print('Done')\n",
    "\n",
    "    def get_rush_sequences_labels(self, week_id = 'all', play_id = 'all', normalize = True, replace_player = None):\n",
    "        if (play_id == 'all') & (replace_player != None):\n",
    "            print('Can only replace player for outputs of one play')\n",
    "            return None\n",
    "        if ( (week_id != 'all') & (play_id != 'all') ):\n",
    "            print('Filter on EITHER week or playId but not both.')\n",
    "            return None\n",
    "        if (replace_player != None):\n",
    "            play_week = self.search_track_weeks(variables = [\"playId\"], variable_values = [play_id]).reset_index(drop = True).week.tolist()[0]\n",
    "        else:\n",
    "            play_week = None\n",
    "        needed_weeks = list(range(1, week))\n",
    "        progressive_training_team = []\n",
    "        for each_week in needed_weeks:\n",
    "            if normalize == True:\n",
    "                print(f'Getting week {each_week} normalized training data')\n",
    "            else:\n",
    "                print(f'Getting week {each_week} (unnormalized) training data')\n",
    "            try:\n",
    "                if (replace_player == None) or (each_week != play_week):\n",
    "                    this_week_train_dat = self.training_data_team.get(str(each_week))\n",
    "                else:\n",
    "                    this_week_train_dat = self.training_data_team.get(str(each_week))\n",
    "                    replaced_player_play = self.replace_player_with_average(play_id, replace_player)\n",
    "                    this_week_train_dat = this_week_train_dat.loc[this_week_train_dat.playId != play_id]\n",
    "                    this_week_train_dat = pd.concat([this_week_train_dat, replaced_player_play])\n",
    "            except:\n",
    "                print(f'Training data has not been loaded for week {str(each_week)}')\n",
    "            progressive_training_team.append(this_week_train_dat)\n",
    "\n",
    "        all_training = pd.concat(progressive_training_team)\n",
    "\n",
    "        if normalize == True:\n",
    "                scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "                scale_cols = all_training.drop(['time_after_snap', 'playId', 'week', 'disrupt_team'], axis = 1)\n",
    "                non_scale_cols = all_training.loc[:,['time_after_snap', 'playId', 'week', 'disrupt_team']]\n",
    "                columns = scale_cols.columns\n",
    "                scale_cols = pd.DataFrame(scaler.fit_transform(scale_cols))\n",
    "                scale_cols.columns = columns\n",
    "                all_training = pd.concat([non_scale_cols.reset_index(drop = 1), scale_cols.reset_index(drop = 1)], axis = 1)\n",
    "\n",
    "        if week_id != 'all':\n",
    "            all_training = all_training.loc[all_training.week == week_id]\n",
    "        if play_id != 'all':\n",
    "            all_training = all_training.loc[all_training.playId == play_id]\n",
    "\n",
    "        sequences = []\n",
    "        for playId, group in tqdm.tqdm(all_training.groupby(\"playId\")):\n",
    "            label = group.iloc[0].disrupt_team\n",
    "            sequence_features = group.drop(['time_after_snap', 'playId', 'week', 'disrupt_team'], axis = 1)\n",
    "            sequences.append((sequence_features, label))\n",
    "        return(sequences)\n",
    "\n",
    "\n",
    "    def replace_player_with_average(self, play_id, player_id): # might need to add normalize to this\n",
    "\n",
    "        play_week = all_data.search_track_weeks(variables = [\"playId\"], variable_values = [play_id]).reset_index(drop = True).week.tolist()[0]\n",
    "        all_week = all_data.training_data_individual.get(str(play_week))\n",
    "        play_dat_individual = all_week.loc[(all_week.playId == play_id)]\n",
    "        lined_up_pos = play_dat_individual.loc[(play_dat_individual.nflId == player_id)].pff_positionLinedUp.tolist()[0]\n",
    "        num_blockers = play_dat_individual.loc[(play_dat_individual.nflId == player_id)].number_blockers_on_play.tolist()[0]\n",
    "        max_time_after_snap = max(play_dat_individual.loc[(play_dat_individual.nflId == player_id)].time_after_snap.tolist())\n",
    "        current_player = play_dat_individual.loc[(play_dat_individual.nflId == player_id)].copy()\n",
    "        current_player_keep = current_player.loc[:,['gameId', 'playId', 'nflId', 'team', 'week', 'x', \n",
    "                                                    'y', 'x_qb', 'y_qb', 's_qb', 'a_qb', 'disrupt_individual']].reset_index(drop = 1)\n",
    "\n",
    "\n",
    "        all_averages = all_data.individual_play_avgs.get(str(play_week))\n",
    "        condition_average = all_averages.loc[(all_averages.pff_positionLinedUp == lined_up_pos) & \n",
    "                                                (all_averages.number_blockers_on_play == num_blockers) &\n",
    "                                                (all_averages.time_after_snap <= max_time_after_snap)].reset_index(drop = 1).rename({'blockers_left_median' : 'blockers_left', \n",
    "                                                                    'rusher_velocity_towards_qb_average' : 'rusher_velocity_towards_qb',\n",
    "                                                                    'rusher_acceleration_towards_qb_average' : 'rusher_acceleration_towards_qb', \n",
    "                                                                    'rusher_distance_from_qb_average' : 'rusher_distance_from_qb', \n",
    "                                                                    'rusher_distance_toward_qb_gained_average' : 'rusher_distance_toward_qb_gained',\n",
    "                                                                    'distance_of_closest_blocker_in_front_average' : 'distance_of_closest_blocker_in_front'}, axis = 1)\n",
    "\n",
    "        replace_averages = pd.concat([condition_average, current_player_keep], axis = 1).reset_index(drop = 1)\n",
    "        removed_player = play_dat_individual.loc[~(play_dat_individual.nflId == player_id)].reset_index(drop = 1)\n",
    "        replaced = pd.concat([replace_averages, removed_player])\n",
    "        replaced_team = replaced.groupby(['playId', 'time_after_snap'], as_index=False).agg({\n",
    "                                    'nflId' : pd.Series.nunique,\n",
    "                                    's_qb' : [min],\n",
    "                                    'a_qb' : [min],\n",
    "                                    'blockers_left' : [min, max, sum],\n",
    "                                    'rusher_velocity_towards_qb' : [min, max, np.average],\n",
    "                                    'rusher_acceleration_towards_qb' : [min, max, np.average],\n",
    "                                    \"rusher_distance_from_qb\": [min, max, np.average],\n",
    "                                    \"rusher_distance_toward_qb_gained\": [min, max, np.average],\n",
    "                                    \"distance_of_closest_blocker_in_front\": [min, max, np.average],\n",
    "                                    \"disrupt_individual\" : [sum],\n",
    "                                    \"week\" : [min]})\n",
    "        replaced_team.columns = [\"_\".join(x) for x in np.array(replaced_team.columns).ravel()]\n",
    "        replaced_team['disrupt_individual_sum'] = replaced_team['disrupt_individual_sum'] > 1\n",
    "        replaced_team = replaced_team.rename(columns = {'playId_' : 'playId', 'time_after_snap_' : 'time_after_snap'}).rename(columns = {'nflId_nunique' : 'n_rushers', \n",
    "                                                                                                                                        's_qb_min' : 's_qb', 'a_qb_min' : 'a_qb',\n",
    "                                                                                                                                        'disrupt_individual_sum' : 'disrupt_team',\n",
    "                                                                                                                                        'week_min' : 'week'})\n",
    "        return(replaced_team)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "####### modeling objects\n",
    "\n",
    "\n",
    "## make dataset object\n",
    "\n",
    "class RushDataset(Dataset):\n",
    "    def __init__(self, sequences, sequence_length):\n",
    "        self.sequences = sequences\n",
    "        self.sequence_length = sequence_length\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.sequences[idx]\n",
    "        label = int(label)\n",
    "        labels = np.repeat(label, self.sequence_length)\n",
    "        if sequence.shape[0] > self.sequence_length:\n",
    "            sequence = sequence.iloc[0:self.sequence_length]\n",
    "        if sequence.shape[0] < self.sequence_length:\n",
    "            # padding = pd.DataFrame(0, index=np.arange(self.sequence_length - sequence.shape[0]), columns=sequence.columns) zero pad does not work\n",
    "            padding = pd.DataFrame(sequence.iloc[-1:].copy())\n",
    "            num_padded = self.sequence_length - sequence.shape[0]\n",
    "            padding_rep = pd.DataFrame(np.repeat(padding.values, num_padded, axis=0)) \n",
    "            padding_rep.columns = sequence.columns\n",
    "            sequence = pd.concat([sequence, padding_rep])\n",
    "\n",
    "        label_tensor = torch.Tensor(labels).long()\n",
    "        data_tensor = torch.Tensor(sequence.values)\n",
    "            \n",
    "        return data_tensor, label_tensor\n",
    "    \n",
    "\n",
    "class SequenceModel(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_classes, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.ltsm = torch.nn.LSTM(input_size = n_features, hidden_size = n_hidden, num_layers = n_layers, batch_first = True)\n",
    "        self.classifier = torch.nn.Linear(n_hidden, n_classes)\n",
    "    def forward(self, x):\n",
    "        unfolded_hiddens, (_, _) = self.ltsm(x)\n",
    "        fc_output = self.classifier(unfolded_hiddens)\n",
    "        return fc_output\n",
    "    def get_layer_probabilities(self, x):\n",
    "        fc_output = self.forward(x)\n",
    "        fc_output_long = fc_output.view([x.size(0) * x.size(1), self.n_classes])\n",
    "        time_prob = torch.nn.functional.softmax(fc_output_long, dim = 1).detach().cpu().numpy().tolist()\n",
    "        time_prob_sack = [x[1] for x in time_prob]\n",
    "        return time_prob_sack\n",
    "\n",
    "\n",
    "\n",
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    outputs = []\n",
    "    labels = []\n",
    "    time_frame_losses = {}\n",
    "    time_frame_labels = {}\n",
    "    time_frame_fc_outputs = {}\n",
    "    i = 0\n",
    "    for X, y in data_loader:\n",
    "        i += 1\n",
    "        fc_output = model(X)\n",
    "        fc_output_long = fc_output.view([data_loader.dataset.sequence_length * X.size(0), model.n_classes])\n",
    "        y_long = y.view([data_loader.dataset.sequence_length * X.size(0)])\n",
    "        outputs.extend([x[0] for x in fc_output_long.detach().numpy().tolist()])\n",
    "        labels.extend(y_long.numpy())\n",
    "        loss = loss_function(fc_output_long, y_long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # get auc per 5 frames\n",
    "\n",
    "        time_frame_start = 0\n",
    "        if data_loader.dataset.sequence_length%5 != 0:\n",
    "            print('Pick a sequence length divisable by 5')\n",
    "            return None\n",
    "        for time_frames in range(int(data_loader.dataset.sequence_length / 5)):\n",
    "            time_frames += 1\n",
    "            this_time_frame_fc_output = fc_output[:, time_frame_start : time_frames*5, :].reshape([X.size(0) * 5, 2])\n",
    "            this_time_frame_ys = y[:, time_frame_start : time_frames*5].reshape([X.size(0) * 5])\n",
    "            loss_this_time_frame = loss_function(this_time_frame_fc_output, this_time_frame_ys)\n",
    "            if i == 1:\n",
    "                time_frame_losses.update({str(time_frames) : [loss_this_time_frame.item()]})\n",
    "                time_frame_labels.update({str(time_frames) : this_time_frame_ys.detach().cpu().numpy().tolist()})\n",
    "                time_frame_fc_outputs.update({str(time_frames) : this_time_frame_fc_output.detach().cpu().numpy().tolist()})\n",
    "            else:\n",
    "                time_frame_losses[str(time_frames)].extend([loss_this_time_frame.item()])\n",
    "                time_frame_labels[str(time_frames)].extend(this_time_frame_ys.detach().cpu().numpy().tolist())\n",
    "                time_frame_fc_outputs[str(time_frames)].extend(this_time_frame_fc_output.detach().cpu().numpy().tolist())\n",
    "            time_frame_start = time_frame_start + 5\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    tpr, fpr, thresholds = sklearn.metrics.roc_curve(y_true = labels, y_score = outputs, pos_label = 1)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    print(f\"Overall Train loss: {avg_loss} , Overall Train AUC: {auc}\")\n",
    "\n",
    "    for time_period in time_frame_losses.keys():\n",
    "        losses = time_frame_losses[time_period]\n",
    "        labels = time_frame_labels[time_period]\n",
    "        outputs_fc = [x[0] for x in time_frame_fc_outputs[time_period]]\n",
    "        \n",
    "        avg_loss = sum(losses) / num_batches\n",
    "        tpr, fpr, thresholds = sklearn.metrics.roc_curve(y_true = labels, y_score = outputs_fc, pos_label = 1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        print(f\"Train loss for period {time_period}: {avg_loss} , Train AUC for period {time_period}: {auc}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        for X, y in data_loader:\n",
    "            i += 1\n",
    "            fc_output = model(X)\n",
    "            fc_output_long = fc_output.view([data_loader.dataset.sequence_length * X.size(0), model.n_classes])\n",
    "            y_long = y.view([data_loader.dataset.sequence_length * X.size(0)])\n",
    "            outputs.extend([x[0] for x in fc_output_long.detach().numpy().tolist()])\n",
    "            labels.extend(y_long.numpy())\n",
    "            total_loss += loss_function(fc_output_long, y_long).item()\n",
    "\n",
    "    time_frame_start = 0\n",
    "    if data_loader.dataset.sequence_length%5 != 0:\n",
    "        print('Pick a sequence length divisable by 5')\n",
    "        return None\n",
    "    for time_frames in range(int(data_loader.dataset.sequence_length / 5)):\n",
    "        time_frames += 1\n",
    "        this_time_frame_fc_output = fc_output[:, time_frame_start : time_frames*5, :].reshape([X.size(0) * 5, 2])\n",
    "        this_time_frame_ys = y[:, time_frame_start : time_frames*5].reshape([X.size(0) * 5])\n",
    "        loss_this_time_frame = loss_function(this_time_frame_fc_output, this_time_frame_ys)\n",
    "        if i == 1:\n",
    "            time_frame_losses.update({str(time_frames) : [loss_this_time_frame.item()]})\n",
    "            time_frame_labels.update({str(time_frames) : this_time_frame_ys.detach().cpu().numpy().tolist()})\n",
    "            time_frame_fc_outputs.update({str(time_frames) : this_time_frame_fc_output.detach().cpu().numpy().tolist()})\n",
    "        else:\n",
    "            time_frame_losses[str(time_frames)].extend([loss_this_time_frame.item()])\n",
    "            time_frame_labels[str(time_frames)].extend(this_time_frame_ys.detach().cpu().numpy().tolist())\n",
    "            time_frame_fc_outputs[str(time_frames)].extend(this_time_frame_fc_output.detach().cpu().numpy().tolist())\n",
    "        time_frame_start = time_frame_start + 5\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    tpr, fpr, thresholds = sklearn.metrics.roc_curve(y_true = labels, y_score = outputs, pos_label = 1)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    print(f\"Overall test loss: {avg_loss} , Overall test AUC: {auc}\")\n",
    "\n",
    "    for time_period in time_frame_losses.keys():\n",
    "        losses = time_frame_losses[time_period]\n",
    "        labels = time_frame_labels[time_period]\n",
    "        outputs_fc = [x[0] for x in time_frame_fc_outputs[time_period]]\n",
    "        \n",
    "        avg_loss = sum(losses) / num_batches\n",
    "        tpr, fpr, thresholds = sklearn.metrics.roc_curve(y_true = labels, y_score = outputs_fc, pos_label = 1)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        print(f\"Test loss for period {time_period}: {avg_loss} , Test AUC for period {time_period}: {auc}\")\n",
    "\n",
    "def train_rush_lstm(train_loader, test_loader, model, loss_function, optimizer, num_epochs = 10):\n",
    "    for ix_epoch in range(num_epochs):\n",
    "        print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "        model = train_model(train_loader, model, loss_function, optimizer=optimizer)\n",
    "        test_model(test_loader, model, loss_function)\n",
    "        print()\n",
    "    return(model)\n",
    "\n",
    "def predict_play(master_track, play_id, model, normalize = True, replace_player = None):\n",
    "    week = all_data.search_track_weeks(variables = [\"playId\"], variable_values = [play_id]).reset_index(drop = True).week.tolist()[0]\n",
    "    play_dat_label = master_track.get_rush_sequences_labels(week = week + 1, normalize = normalize, play_id = play_id, replace_player = replace_player)\n",
    "    play_dataset = RushDataset(sequences = play_dat_label, sequence_length=30)\n",
    "    play_loader = DataLoader(play_dataset, batch_size = 30, shuffle = True)\n",
    "\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, _ in play_loader:\n",
    "            y_star = model.get_layer_probabilities(X)\n",
    "            outputs.extend(y_star)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_metric_averages(self, week):\n",
    "        if self.overall_avgs.get(str(week - 1)) is None:\n",
    "            print(\"Rusher averages are not set to week of this play. Use the load_distance_averages_by_time_after_snap to load proper week averages\")\n",
    "            return(None)\n",
    "        overall_avgs = self.overall_avgs.get(str(week))\n",
    "        plt.plot(overall_avgs.time_after_snap.values, overall_avgs.rusher_distance_to_qb_gained_league_avg.values)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# to do:\n",
    "# think about the fact that metric must be team based and must incoorperate number of rushers\n",
    "# fix updater (ie. week 3 or above)\n",
    "# add qb to tracking plots\n",
    "# create modeling for each play frame\n",
    "# add model prob of event to plots\n",
    "# create funciton to get player metric after week x\n",
    "# create function to get team metric for week x\n",
    "# create function to get result for number of team events and training data\n",
    "# create modeling for team number of events by metric\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b5140b98f9aa636ad904647e184ddf94a8c49b25e448223e4f659e3845abf7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
